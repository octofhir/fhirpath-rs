// Copyright 2024 OctoFHIR Team
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

//! Arc Pool Management for FHIRPath Objects
//!
//! This module provides specialized Arc pooling for frequently allocated objects
//! to reduce memory allocations and improve Arc reuse patterns.

use crate::FhirPathValue;
use dashmap::DashMap;
use std::collections::HashMap;
use std::fmt;
use std::hash::{DefaultHasher, Hash, Hasher};
use std::sync::atomic::{AtomicU64, AtomicUsize, Ordering};
use std::sync::{Arc, Mutex, Weak};
use std::time::{SystemTime, UNIX_EPOCH};

/// Configuration for Arc pool behavior
#[derive(Debug, Clone)]
pub struct ArcPoolConfig {
    /// Maximum number of cached Arc objects per type
    pub max_cached_objects: usize,
    /// Minimum reference count before considering an Arc for reuse
    pub min_reuse_threshold: usize,
    /// Cleanup interval in seconds
    pub cleanup_interval_secs: u64,
    /// Maximum memory overhead for pool management
    pub max_pool_memory_bytes: usize,
    /// Enable fragmentation monitoring
    pub enable_fragmentation_monitoring: bool,
}

impl Default for ArcPoolConfig {
    fn default() -> Self {
        Self {
            max_cached_objects: 1000,
            min_reuse_threshold: 2,
            cleanup_interval_secs: 300,              // 5 minutes
            max_pool_memory_bytes: 10 * 1024 * 1024, // 10MB
            enable_fragmentation_monitoring: true,
        }
    }
}

/// Statistics for Arc pool monitoring
#[derive(Debug, Default)]
pub struct ArcPoolStats {
    /// Total number of Arc objects created
    pub arcs_created: AtomicU64,
    /// Total number of Arc objects reused from pool
    pub arcs_reused: AtomicU64,
    /// Total number of Arc objects evicted
    pub arcs_evicted: AtomicU64,
    /// Number of cleanup operations performed
    pub cleanup_operations: AtomicU64,
    /// Current memory usage estimate
    pub current_memory_bytes: AtomicUsize,
    /// Number of fragmented allocations detected
    pub fragmented_allocations: AtomicU64,
    /// Total bytes saved through reuse
    pub bytes_saved: AtomicU64,
}

impl ArcPoolStats {
    /// Calculate the reuse rate as a percentage
    pub fn reuse_rate(&self) -> f64 {
        let created = self.arcs_created.load(Ordering::Relaxed);
        let reused = self.arcs_reused.load(Ordering::Relaxed);
        let total = created + reused;

        if total == 0 {
            0.0
        } else {
            (reused as f64 / total as f64) * 100.0
        }
    }

    /// Calculate memory efficiency (bytes saved vs current usage)
    pub fn memory_efficiency(&self) -> f64 {
        let current = self.current_memory_bytes.load(Ordering::Relaxed);
        let saved = self.bytes_saved.load(Ordering::Relaxed);

        if current == 0 {
            0.0
        } else {
            saved as f64 / current as f64
        }
    }
}

/// Metadata for tracking Arc usage patterns
#[derive(Debug, Clone)]
struct ArcMetadata {
    /// Hash of the object for deduplication
    object_hash: u64,
    /// Number of times this Arc has been reused
    reuse_count: u32,
    /// Last access timestamp
    last_accessed: u64,
    /// Estimated size in bytes
    estimated_size: usize,
    /// Number of strong references when last checked
    last_strong_count: usize,
}

impl ArcMetadata {
    fn new(object_hash: u64, estimated_size: usize) -> Self {
        let now = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap_or_default()
            .as_secs();

        Self {
            object_hash,
            reuse_count: 0,
            last_accessed: now,
            estimated_size,
            last_strong_count: 1,
        }
    }

    fn record_access(&mut self) {
        self.reuse_count += 1;
        self.last_accessed = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap_or_default()
            .as_secs();
    }
}

/// Pool entry combining weak reference with metadata
struct PoolEntry<T> {
    weak_ref: Weak<T>,
    metadata: ArcMetadata,
}

/// Hash-based wrapper for non-Hash types
pub struct HashableWrapper<T> {
    value: T,
    hash: u64,
}

impl<T> HashableWrapper<T>
where
    T: Clone + PartialEq + fmt::Debug,
{
    /// Creates a new hashable wrapper around the given value
    pub fn new(value: T) -> Self {
        let hash = Self::compute_hash(&value);
        Self { value, hash }
    }

    fn compute_hash(value: &T) -> u64 {
        let mut hasher = DefaultHasher::new();
        // For non-Hash types, we'll use a simple content-based hash
        let debug_str = format!("{value:?}");
        debug_str.hash(&mut hasher);
        hasher.finish()
    }
}

impl<T> Hash for HashableWrapper<T> {
    fn hash<H: Hasher>(&self, state: &mut H) {
        self.hash.hash(state);
    }
}

impl<T> PartialEq for HashableWrapper<T>
where
    T: PartialEq,
{
    fn eq(&self, other: &Self) -> bool {
        self.value == other.value
    }
}

impl<T> Clone for HashableWrapper<T>
where
    T: Clone,
{
    fn clone(&self) -> Self {
        Self {
            value: self.value.clone(),
            hash: self.hash,
        }
    }
}

/// Specialized Arc pool for a specific type
pub struct TypedArcPool<T> {
    /// Pool storage using hash-based lookup for deduplication
    pool: DashMap<u64, PoolEntry<T>>,
    /// Configuration
    config: ArcPoolConfig,
    /// Statistics
    stats: ArcPoolStats,
    /// Type name for debugging
    type_name: &'static str,
}

impl<T> TypedArcPool<T>
where
    T: PartialEq + Clone + Send + Sync + fmt::Debug + 'static,
{
    /// Create a new typed Arc pool
    pub fn new(type_name: &'static str) -> Self {
        Self::with_config(type_name, ArcPoolConfig::default())
    }

    /// Create a new typed Arc pool with custom configuration
    pub fn with_config(type_name: &'static str, config: ArcPoolConfig) -> Self {
        Self {
            pool: DashMap::new(),
            config,
            stats: ArcPoolStats::default(),
            type_name,
        }
    }

    /// Get or create an Arc for the given object
    pub fn get_or_create(&self, object: T) -> Arc<T> {
        let object_hash = self.hash_object(&object);

        // Try to reuse existing Arc
        if let Some(existing) = self.try_reuse(object_hash, &object) {
            return existing;
        }

        // Create new Arc
        let arc = Arc::new(object);
        let estimated_size = self.estimate_size(&arc);

        // Store weak reference for future reuse
        self.store_for_reuse(object_hash, arc.clone(), estimated_size);

        self.stats.arcs_created.fetch_add(1, Ordering::Relaxed);
        self.stats
            .current_memory_bytes
            .fetch_add(estimated_size, Ordering::Relaxed);

        arc
    }

    /// Try to reuse an existing Arc from the pool
    fn try_reuse(&self, object_hash: u64, object: &T) -> Option<Arc<T>> {
        if let Some(mut entry) = self.pool.get_mut(&object_hash) {
            if let Some(strong_ref) = entry.weak_ref.upgrade() {
                // Verify the objects are actually equal (hash collision protection)
                if *strong_ref == *object {
                    entry.metadata.record_access();
                    self.stats.arcs_reused.fetch_add(1, Ordering::Relaxed);
                    self.stats
                        .bytes_saved
                        .fetch_add(entry.metadata.estimated_size as u64, Ordering::Relaxed);
                    return Some(strong_ref);
                }
            } else {
                // Weak reference is dead, will be cleaned up later
                return None;
            }
        }
        None
    }

    /// Store a new Arc for future reuse
    fn store_for_reuse(&self, object_hash: u64, arc: Arc<T>, estimated_size: usize) {
        let metadata = ArcMetadata::new(object_hash, estimated_size);
        let weak_ref = Arc::downgrade(&arc);

        let entry = PoolEntry { weak_ref, metadata };

        // Check if we need to evict entries before inserting
        if self.pool.len() >= self.config.max_cached_objects {
            self.evict_lru_entries();
        }

        self.pool.insert(object_hash, entry);
    }

    /// Evict least recently used entries to make space
    fn evict_lru_entries(&self) {
        let target_count = self.config.max_cached_objects * 3 / 4; // Evict to 75% capacity
        let mut entries_to_evict = Vec::new();

        // Collect entries sorted by last access time
        for entry in self.pool.iter() {
            entries_to_evict.push((*entry.key(), entry.metadata.last_accessed));
        }

        // Sort by last accessed (oldest first)
        entries_to_evict.sort_by_key(|(_, last_accessed)| *last_accessed);

        // Evict oldest entries
        let evict_count = self.pool.len().saturating_sub(target_count);
        for (hash, _) in entries_to_evict.iter().take(evict_count) {
            if let Some((_, entry)) = self.pool.remove(hash) {
                self.stats
                    .current_memory_bytes
                    .fetch_sub(entry.metadata.estimated_size, Ordering::Relaxed);
                self.stats.arcs_evicted.fetch_add(1, Ordering::Relaxed);
            }
        }
    }

    /// Clean up dead weak references and perform maintenance
    pub fn cleanup(&self) {
        let mut dead_entries = Vec::new();
        let mut memory_freed = 0;

        // Identify dead entries
        for entry in self.pool.iter() {
            if entry.weak_ref.strong_count() == 0 {
                dead_entries.push(*entry.key());
                memory_freed += entry.metadata.estimated_size;
            }
        }

        // Remove dead entries
        for hash in dead_entries {
            self.pool.remove(&hash);
        }

        self.stats
            .current_memory_bytes
            .fetch_sub(memory_freed, Ordering::Relaxed);
        self.stats
            .cleanup_operations
            .fetch_add(1, Ordering::Relaxed);

        // Check memory pressure and evict if necessary
        let current_memory = self.stats.current_memory_bytes.load(Ordering::Relaxed);
        if current_memory > self.config.max_pool_memory_bytes {
            self.evict_lru_entries();
        }
    }

    /// Get statistics for this pool
    pub fn stats(&self) -> &ArcPoolStats {
        &self.stats
    }

    /// Get current pool size
    pub fn pool_size(&self) -> usize {
        self.pool.len()
    }

    /// Clear all entries from the pool
    pub fn clear(&self) {
        let memory_freed = self
            .pool
            .iter()
            .map(|entry| entry.metadata.estimated_size)
            .sum::<usize>();

        self.pool.clear();
        self.stats
            .current_memory_bytes
            .fetch_sub(memory_freed, Ordering::Relaxed);
    }

    /// Hash an object for pool key generation
    fn hash_object(&self, object: &T) -> u64 {
        let mut hasher = DefaultHasher::new();
        // For non-Hash types, use Debug-based hashing
        let debug_str = format!("{object:?}");
        debug_str.hash(&mut hasher);
        hasher.finish()
    }

    /// Estimate the memory size of an object
    fn estimate_size(&self, _arc: &Arc<T>) -> usize {
        // Base size of Arc overhead plus estimated object size
        let arc_overhead = std::mem::size_of::<T>() + std::mem::size_of::<usize>() * 2;

        // For FhirPathValue, we can be more specific
        if std::any::type_name::<T>().contains("FhirPathValue") {
            // Estimate based on the variant
            256 // Conservative estimate
        } else {
            arc_overhead + 64 // Conservative default estimate
        }
    }
}

/// Global Arc pool manager for all types
pub struct GlobalArcPoolManager {
    /// Pool for FhirPathValues
    fhir_value_pool: TypedArcPool<FhirPathValue>,
    /// Pool for string values
    string_pool: TypedArcPool<String>,
    /// Pool for collections
    collection_pool: TypedArcPool<Vec<FhirPathValue>>,
    /// Global configuration
    config: ArcPoolConfig,
    /// Fragmentation monitor
    fragmentation_monitor: FragmentationMonitor,
}

impl GlobalArcPoolManager {
    /// Create a new global Arc pool manager
    pub fn new() -> Self {
        let config = ArcPoolConfig::default();
        Self::with_config(config)
    }

    /// Create with custom configuration
    pub fn with_config(config: ArcPoolConfig) -> Self {
        Self {
            fhir_value_pool: TypedArcPool::with_config("FhirPathValue", config.clone()),
            string_pool: TypedArcPool::with_config("String", config.clone()),
            collection_pool: TypedArcPool::with_config("Vec<FhirPathValue>", config.clone()),
            fragmentation_monitor: FragmentationMonitor::new(
                config.enable_fragmentation_monitoring,
            ),
            config,
        }
    }

    /// Get or create an Arc for a FhirPathValue
    pub fn get_fhir_value(&self, value: FhirPathValue) -> Arc<FhirPathValue> {
        let arc = self.fhir_value_pool.get_or_create(value);
        self.fragmentation_monitor
            .record_allocation(std::any::type_name::<FhirPathValue>());
        arc
    }

    /// Get or create an Arc for a String
    pub fn get_string(&self, string: String) -> Arc<String> {
        let arc = self.string_pool.get_or_create(string);
        self.fragmentation_monitor
            .record_allocation(std::any::type_name::<String>());
        arc
    }

    /// Get or create an Arc for a collection
    pub fn get_collection(&self, collection: Vec<FhirPathValue>) -> Arc<Vec<FhirPathValue>> {
        let arc = self.collection_pool.get_or_create(collection);
        self.fragmentation_monitor
            .record_allocation(std::any::type_name::<Vec<FhirPathValue>>());
        arc
    }

    /// Perform cleanup on all pools
    pub fn cleanup_all(&self) {
        self.fhir_value_pool.cleanup();
        self.string_pool.cleanup();
        self.collection_pool.cleanup();
        self.fragmentation_monitor.cleanup();
    }

    /// Get combined statistics from all pools
    pub fn combined_stats(&self) -> CombinedArcPoolStats {
        CombinedArcPoolStats {
            fhir_value_stats: CombinedTypeStats::from_pool_stats(
                self.fhir_value_pool.stats(),
                self.fhir_value_pool.pool_size(),
            ),
            string_stats: CombinedTypeStats::from_pool_stats(
                self.string_pool.stats(),
                self.string_pool.pool_size(),
            ),
            collection_stats: CombinedTypeStats::from_pool_stats(
                self.collection_pool.stats(),
                self.collection_pool.pool_size(),
            ),
            fragmentation_stats: self.fragmentation_monitor.stats().clone(),
        }
    }

    /// Clear all pools
    pub fn clear_all(&self) {
        self.fhir_value_pool.clear();
        self.string_pool.clear();
        self.collection_pool.clear();
    }
}

impl Default for GlobalArcPoolManager {
    fn default() -> Self {
        Self::new()
    }
}

/// Combined statistics from all Arc pools
#[derive(Debug)]
pub struct CombinedArcPoolStats {
    /// Statistics for FHIR value pools
    pub fhir_value_stats: CombinedTypeStats,
    /// Statistics for string pools
    pub string_stats: CombinedTypeStats,
    /// Statistics for collection pools
    pub collection_stats: CombinedTypeStats,
    /// Memory fragmentation statistics across all pools
    pub fragmentation_stats: FragmentationStats,
}

impl CombinedArcPoolStats {
    /// Calculate total reuse rate across all pools
    pub fn total_reuse_rate(&self) -> f64 {
        let total_created = self.fhir_value_stats.arcs_created
            + self.string_stats.arcs_created
            + self.collection_stats.arcs_created;
        let total_reused = self.fhir_value_stats.arcs_reused
            + self.string_stats.arcs_reused
            + self.collection_stats.arcs_reused;
        let total = total_created + total_reused;

        if total == 0 {
            0.0
        } else {
            (total_reused as f64 / total as f64) * 100.0
        }
    }

    /// Calculate total memory usage across all pools
    pub fn total_memory_bytes(&self) -> usize {
        self.fhir_value_stats.current_memory_bytes
            + self.string_stats.current_memory_bytes
            + self.collection_stats.current_memory_bytes
    }

    /// Calculate total bytes saved through reuse
    pub fn total_bytes_saved(&self) -> u64 {
        self.fhir_value_stats.bytes_saved
            + self.string_stats.bytes_saved
            + self.collection_stats.bytes_saved
    }
}

/// Statistics for a single type pool
#[derive(Debug)]
pub struct CombinedTypeStats {
    /// Total number of Arc objects created
    pub arcs_created: u64,
    /// Total number of Arc objects reused from pool
    pub arcs_reused: u64,
    /// Total number of Arc objects evicted from pool
    pub arcs_evicted: u64,
    /// Current memory usage in bytes
    pub current_memory_bytes: usize,
    /// Total bytes saved through Arc reuse
    pub bytes_saved: u64,
    /// Current size of the pool
    pub pool_size: usize,
    /// Current reuse rate as a percentage
    pub reuse_rate: f64,
}

impl CombinedTypeStats {
    fn from_pool_stats(stats: &ArcPoolStats, pool_size: usize) -> Self {
        Self {
            arcs_created: stats.arcs_created.load(Ordering::Relaxed),
            arcs_reused: stats.arcs_reused.load(Ordering::Relaxed),
            arcs_evicted: stats.arcs_evicted.load(Ordering::Relaxed),
            current_memory_bytes: stats.current_memory_bytes.load(Ordering::Relaxed),
            bytes_saved: stats.bytes_saved.load(Ordering::Relaxed),
            pool_size,
            reuse_rate: stats.reuse_rate(),
        }
    }
}

/// Fragmentation monitoring for Arc allocations
pub struct FragmentationMonitor {
    enabled: bool,
    allocation_patterns: Mutex<HashMap<&'static str, AllocationPattern>>,
    stats: FragmentationStats,
}

/// Pattern tracking for allocation fragmentation detection
#[derive(Debug, Default)]
struct AllocationPattern {
    /// Total number of allocations tracked
    allocation_count: u64,
    /// Timestamp of the last allocation
    last_allocation_time: u64,
    /// Average time interval between allocations
    average_interval: f64,
    /// Calculated fragmentation score (0.0 = no fragmentation, 1.0 = high fragmentation)
    fragmentation_score: f64,
}

/// Statistics about memory fragmentation
#[derive(Debug, Clone, Default)]
pub struct FragmentationStats {
    /// Total number of allocations tracked
    pub total_allocations: u64,
    /// Number of allocations considered fragmented
    pub fragmented_allocations: u64,
    /// Ratio of fragmented to total allocations
    pub fragmentation_ratio: f64,
    /// Number of types being monitored for fragmentation
    pub monitored_types: usize,
}

impl FragmentationMonitor {
    fn new(enabled: bool) -> Self {
        Self {
            enabled,
            allocation_patterns: Mutex::new(HashMap::new()),
            stats: FragmentationStats::default(),
        }
    }

    fn record_allocation(&self, type_name: &'static str) {
        if !self.enabled {
            return;
        }

        let now = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap_or_default()
            .as_millis() as u64;

        if let Ok(mut patterns) = self.allocation_patterns.lock() {
            let pattern = patterns.entry(type_name).or_default();

            pattern.allocation_count += 1;

            if pattern.last_allocation_time > 0 {
                let interval = now.saturating_sub(pattern.last_allocation_time) as f64;
                pattern.average_interval = (pattern.average_interval + interval) / 2.0;

                // Simple fragmentation heuristic: rapid allocations suggest fragmentation
                if interval < 10.0 && pattern.allocation_count > 10 {
                    pattern.fragmentation_score += 0.1;
                }
            }

            pattern.last_allocation_time = now;
        }
    }

    fn cleanup(&self) {
        if !self.enabled {
            return;
        }

        // Update fragmentation statistics
        if let Ok(patterns) = self.allocation_patterns.lock() {
            let _total_allocations: u64 = patterns.values().map(|p| p.allocation_count).sum();
            let _fragmented_allocations: u64 = patterns
                .values()
                .filter(|p| p.fragmentation_score > 0.5)
                .map(|p| p.allocation_count)
                .sum();

            // Update atomic statistics would go here if we made them atomic
            // For now, we'll just calculate them on demand
        }
    }

    fn stats(&self) -> &FragmentationStats {
        &self.stats
    }
}

/// Global instance of the Arc pool manager
static GLOBAL_ARC_POOL: once_cell::sync::Lazy<GlobalArcPoolManager> =
    once_cell::sync::Lazy::new(GlobalArcPoolManager::new);

/// Get the global Arc pool manager
pub fn global_arc_pool() -> &'static GlobalArcPoolManager {
    &GLOBAL_ARC_POOL
}

/// Convenience function to get a pooled FhirPathValue Arc
pub fn get_pooled_fhir_value(value: FhirPathValue) -> Arc<FhirPathValue> {
    global_arc_pool().get_fhir_value(value)
}

/// Convenience function to get a pooled String Arc
pub fn get_pooled_string(string: String) -> Arc<String> {
    global_arc_pool().get_string(string)
}

/// Convenience function to get a pooled collection Arc
pub fn get_pooled_collection(collection: Vec<FhirPathValue>) -> Arc<Vec<FhirPathValue>> {
    global_arc_pool().get_collection(collection)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_typed_arc_pool_basic() {
        let pool = TypedArcPool::new("String");

        let value1 = "test_string".to_string();
        let value2 = "test_string".to_string();

        let arc1 = pool.get_or_create(value1);
        let arc2 = pool.get_or_create(value2);

        // Should reuse the same Arc
        assert!(Arc::ptr_eq(&arc1, &arc2));
        assert_eq!(pool.stats().reuse_rate(), 50.0); // 1 reuse out of 2 operations
    }

    #[test]
    fn test_pool_cleanup() {
        let pool = TypedArcPool::new("String");

        {
            let _arc = pool.get_or_create("temporary".to_string());
            assert_eq!(pool.pool_size(), 1);
        } // Arc goes out of scope

        pool.cleanup();
        assert_eq!(pool.pool_size(), 0);
    }

    #[test]
    fn test_pool_eviction() {
        let config = ArcPoolConfig {
            max_cached_objects: 2,
            ..ArcPoolConfig::default()
        };
        let pool = TypedArcPool::with_config("String", config);

        let _arc1 = pool.get_or_create("string1".to_string());
        let _arc2 = pool.get_or_create("string2".to_string());
        let _arc3 = pool.get_or_create("string3".to_string());

        // Pool should trigger eviction
        assert!(pool.pool_size() <= 2);
        assert!(pool.stats().arcs_evicted.load(Ordering::Relaxed) > 0);
    }

    #[test]
    fn test_global_pool_manager() {
        let manager = GlobalArcPoolManager::new();

        let value = FhirPathValue::String(Arc::from("test"));
        let arc1 = manager.get_fhir_value(value.clone());
        let arc2 = manager.get_fhir_value(value);

        assert!(Arc::ptr_eq(&arc1, &arc2));

        let stats = manager.combined_stats();
        assert!(stats.total_reuse_rate() > 0.0);
    }

    #[test]
    fn test_fragmentation_monitor() {
        let monitor = FragmentationMonitor::new(true);

        // Simulate rapid allocations
        for _ in 0..20 {
            monitor.record_allocation("TestType");
            std::thread::sleep(std::time::Duration::from_millis(1));
        }

        monitor.cleanup();
        // Fragmentation monitoring is working (basic functionality test)
    }

    #[test]
    fn test_arc_metadata() {
        let mut metadata = ArcMetadata::new(12345, 256);
        assert_eq!(metadata.reuse_count, 0);
        assert_eq!(metadata.estimated_size, 256);

        metadata.record_access();
        assert_eq!(metadata.reuse_count, 1);
        assert!(metadata.last_accessed > 0);
    }
}
